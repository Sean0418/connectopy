---
title: "Final Project - Option 2"
date: "`r Sys.Date()`"
bibliography: paperdti.bib
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE, autodep=TRUE, cache.comments=TRUE)
```




```{r load_data, echo=TRUE, message=FALSE}
library(R.matlab)
library(dplyr)
library(ggplot2)

# --- 1. Load Structural Data (TNPCA Coefficients) ---
struct_path <- "Data/TNPCA_Result/TNPCA_Coeff_HCP_Structural_Connectome.mat"
struct_data <- readMat(struct_path)

func_path <- "Data/TNPCA_Result/TNPCA_Coeff_HCP_Functional_Connectome.mat" 
func_data <- readMat(func_path)


# We use 'matrix()' to flatten it correctly, preserving the subject order
struct_coeffs <- matrix(struct_data$PCA.Coeff, nrow = 1065, ncol = 60)
struct_ids    <- as.vector(struct_data$sub.id)

# Name the columns PC1...PC60 immediately
colnames(struct_coeffs) <- paste0("PC", 1:60)

# Create a clean dataframe for the brain features
brain_df <- data.frame(Subject = struct_ids, struct_coeffs)

func_coeffs <- matrix(func_data$PCA.Coeff, nrow = 1058, ncol = 60)
func_ids <- as.vector(func_data$network.subject.ids)
colnames(func_coeffs) <- paste0("Func_PC", 1:60) 
func_df <- data.frame(Subject = func_ids, func_coeffs)

colnames(brain_df) <- c("Subject", colnames(struct_coeffs))

# --- 2. Load Trait Data ---
traits_path <- "Data/traits/table1_hcp.csv"
traits_df   <- read.csv(traits_path)
traits2_path <- "Data/traits/table2_hcp.csv" 
traits2_df   <- read.csv(traits2_path)

# --- 3. Merge ---
all_traits <- inner_join(traits_df, traits2_df, by = "Subject")
# Inner join ensures we match traits to the correct brain scan
half_data <- inner_join(all_traits, brain_df, by = "Subject")
full_data <- inner_join(half_data, func_df, by = "Subject")

# Verification: Variance should be > 0 (meaning points are different)
print(paste("Data Loaded. Rows:", nrow(full_data)))
print(paste("PC1 Variance:", var(full_data$PC1)))
```

<<<<<<< Updated upstream
```{r scree_plot, echo=TRUE}
# Calculate variance for each of the 60 PCs
pc_variances <- apply(full_data[, paste0("PC", 1:60)], 2, var)

# Calculate percentage of total variance explained by these 60 PCs
pc_percent <- (pc_variances / sum(pc_variances)) * 100

# Create a dataframe for plotting
scree_df <- data.frame(
  PC_Index = 1:60,
  Variance_Explained = pc_percent
)

# Plot
ggplot(scree_df, aes(x = PC_Index, y = Variance_Explained)) +
  geom_line(color = "blue") +
  geom_point(color = "red", size = 2) +
  theme_minimal() +
  labs(title = "Scree Plot (Variance Explained by Each PC)",
       y = "Percentage of Variance (%)",
       x = "Principal Component Index")
```
=======

>>>>>>> Stashed changes

```{r viz_pca, echo=TRUE, message=FALSE, warning=FALSE}
# --- 1. Plot PC1 vs PC2 colored by Gender ---
p1 <- ggplot(full_data, aes(x = PC1, y = PC2, color = Gender)) +
  geom_point(alpha = 0.7, size = 2) +
  stat_ellipse(geom = "polygon", alpha = 0.2, level = 0.95)+
  theme_minimal() +
  labs(title = "Brain Structure: Male vs Female",
       subtitle = "Colored by Gender")

# --- 2. Plot PC1 vs PC2 colored by Age ---
p2 <- ggplot(full_data, aes(x = PC1, y = PC2, color = Age)) +
  geom_point(alpha = 0.7, size = 2) +
  stat_ellipse(geom = "polygon", alpha = 0.2, level = 0.95)+
  theme_minimal() +
  labs(title = "Brain Structure: Age Groups",
       subtitle = "Colored by Age Range")

# Print plots
print(p1)
print(p2)
```

<<<<<<< Updated upstream
```{r hypothesis_test, echo=TRUE}
# 1. Visual Check with Boxplots (Better than scatter for 2 groups)
ggplot(full_data, aes(x = Gender, y = PC1, fill = Gender)) +
  geom_boxplot() +
  theme_minimal() +
  labs(title = "Distribution of PC1 by Gender")

# 2. Statistical T-Test
# Null Hypothesis: The mean PC1 score is the same for Males and Females
t_test_result <- t.test(PC1 ~ Gender, data = full_data)

print(t_test_result)
```

```{r prediction_model, echo=TRUE}
# Let's try to predict the Age Group using the top 10 PCs
# We use a simple linear model treating Age groups as factors

# 1. Build the model
# Formula: Age depends on PC1 + PC2 + ... + PC10
model <- lm(as.numeric(as.factor(Age)) ~ PC1 + PC2 + PC3 + PC4 + PC5, data = full_data)

# 2. Check summary
summary(model)
```
```{r viz_age_pc5, echo=TRUE}
# Your model showed PC5 is significant for Age, so let's plot it
# We use boxplots to see if the median PC5 score shifts as people get older
ggplot(full_data, aes(x = Age, y = PC5, fill = Age)) +
  geom_boxplot() +
  theme_minimal() +
  labs(title = "Why PC5 predicts Age: Distribution by Age Group",
       subtitle = "Notice the shift in the median (middle line) across groups",
       y = "PC5 Score")
```

```{r rf_model, echo=TRUE}
# Install if needed: install.packages("randomForest")
library(randomForest)

# 1. Prepare Data
# Random Forest works best with "Factors" for classification
# We will try to predict the Age Group using ALL 60 PCs
set.seed(123) # meaningful seed for reproducibility

# 2. Train the Model
# "Age ~ ." means predict Age using all other columns in 'rf_data'
# We subset to just Age and the PCs to keep it clean
rf_data <- full_data[, c("Age", paste0("PC", 1:60))]
rf_model <- randomForest(as.factor(Age) ~ ., data = rf_data, ntree=500)

# 3. Check Results
print(rf_model)

# 4. Which PCs matter most?
varImpPlot(rf_model, n.var=10, main="Top 10 Important PCs for Predicting Age")
```

```{r rf_merge_classes, echo=TRUE}
library(dplyr)
library(randomForest)

# 1. Create a new dataframe with merged Age groups
rf_data_merged <- full_data[, c("Age", paste0("PC", 1:60))]

# Convert Age to character so we can edit it, then merge
rf_data_merged$Age <- as.character(rf_data_merged$Age)
rf_data_merged$Age[rf_data_merged$Age == "36+"] <- "31-35"
rf_data_merged$Age <- as.factor(rf_data_merged$Age)

# Rename "31-35" to "31+" to be accurate
levels(rf_data_merged$Age)[levels(rf_data_merged$Age)=="31-35"] <- "31+"

# Check new counts
print("New Class Counts:")
print(table(rf_data_merged$Age))

# 2. Run Random Forest on the merged data
set.seed(123)
rf_model_merged <- randomForest(Age ~ ., data = rf_data_merged, ntree=500)

print(rf_model_merged)
```

```{r rf_balanced, echo=TRUE}
library(randomForest)

# 1. Check the smallest class size
# We know it is 10 (from the 36+ group)
min_size <- 10 

# 2. Run Weighted/Balanced Random Forest
# sampsize: We instruct it to pick 10 random samples from EACH group for every tree.
# strata: We tell it to stratify by Age.
set.seed(123)
rf_model_balanced <- randomForest(as.factor(Age) ~ ., 
                                  data = rf_data, 
                                  ntree=500,
                                  strata = as.factor(rf_data$Age),
                                  sampsize = c(10, 10, 10, 10)) # 10 from each of the 4 groups

# 3. Check Results
print(rf_model_balanced)
```

```{r final_heatmap, echo=TRUE}
library(reshape2)
library(R.matlab)
library(ggplot2)

# Load the raw connectivity matrices
raw_data <- readMat("Data/SC/HCP_cortical_DesikanAtlas_SC.mat")

# The variable in the mat file is 'hcp_sc_count', which R converts to 'hcp.sc.count'
connectivity <- raw_data$hcp.sc.count

# Plot Subject 1
# We take the 1st slice of the 3D array
subj1_matrix <- log1p(connectivity[,,1]) 
melted_c <- melt(subj1_matrix)

ggplot(melted_c, aes(Var1, Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "darkred") +
  coord_fixed() +
  theme_minimal() +
  labs(title = "Structural Connectome (Subject 1)", x="Region", y="Region") +
  theme(axis.text=element_blank())
```

```{r load_dual_data, echo=TRUE, message=FALSE}
library(R.matlab)
library(dplyr)
library(randomForest)

# --- 1. Load Structural (1065 Subjects) ---
s_data <- readMat("Data/TNPCA_Result/TNPCA_Coeff_HCP_Structural_Connectome.mat")
# Get number of subjects dynamically
n_subs_struct <- length(s_data$sub.id)
s_coeffs <- matrix(s_data$PCA.Coeff, nrow = n_subs_struct, ncol = 60)
colnames(s_coeffs) <- paste0("Struct_PC", 1:60)
df_struct <- data.frame(Subject = as.vector(s_data$sub.id), s_coeffs)

# --- 2. Load Functional (1058 Subjects - FIX HERE) ---
f_data <- readMat("Data/TNPCA_Result/TNPCA_Coeff_HCP_Functional_Connectome.mat")
# Get number of subjects dynamically (It is 1058, not 1065)
n_subs_func <- length(f_data$network.subject.ids)
f_coeffs <- matrix(f_data$PCA.Coeff, nrow = n_subs_func, ncol = 60)
colnames(f_coeffs) <- paste0("Func_PC", 1:60)
df_func <- data.frame(Subject = as.vector(f_data$network.subject.ids), f_coeffs)

# --- 3. Merge Everything ---
traits <- read.csv("Data/traits/table1_hcp.csv")

# Inner join will automatically keep only the subjects present in ALL files
# This results in the intersection (1058 subjects)
mega_data <- inner_join(traits, df_struct, by = "Subject") %>%
             inner_join(df_func, by = "Subject")

print(paste("Mega Dataset Loaded. Rows:", nrow(mega_data), "| Features:", ncol(mega_data)))
```

```{r rf_boosted, echo=TRUE}
# 1. Clean Age Data
rf_data <- mega_data[, c("Age", grep("PC", colnames(mega_data), value=TRUE))]
rf_data$Age <- as.factor(rf_data$Age)

# 2. Manual Oversampling
# Instead of throwing away data, we duplicate the rare rows
# Target size: ~460 (Size of the "26-30" group)

grp_36 <- rf_data[rf_data$Age == "36+", ]
grp_22 <- rf_data[rf_data$Age == "22-25", ]
grp_31 <- rf_data[rf_data$Age == "31-35", ]

# Replicate 36+ (Need ~45x more)
boost_36 <- grp_36[rep(1:nrow(grp_36), 45), ]

# Replicate 22-25 (Need ~2x more)
boost_22 <- grp_22[rep(1:nrow(grp_22), 2), ]

# Replicate 31-35 (Need ~1.5x more)
boost_31 <- grp_31[rep(1:nrow(grp_31), 1), ] # Just adding 1x copy for slight boost

# Combine into a "Balanced Training Set"
train_balanced <- rbind(rf_data, boost_36, boost_22, boost_31)

print("Balanced Class Counts:")
print(table(train_balanced$Age))

# 3. Train Model on Super-Set
set.seed(999)
rf_super <- randomForest(Age ~ ., data = train_balanced, ntree=1000, mtry=15)

print(rf_super)

# 4. See what matters (Structural vs Functional)
varImpPlot(rf_super, n.var=15, main="Top Predictors: Structure vs Function")
```

```{r rf_gold_standard_fixed, echo=TRUE}
library(caret)
library(randomForest)

set.seed(42)

# --- 1. CLEANING STEP (Missing in your code) ---
# Select only Age and the PC columns (removing Subject ID and metadata with NAs)
rf_data <- mega_data[, c("Age", grep("PC", colnames(mega_data), value=TRUE))]

# Ensure Age is a Factor (required for Classification)
rf_data$Age <- as.factor(rf_data$Age)

# Check for NAs just in case (should be 0 now)
sum(is.na(rf_data)) 

# --- 2. Create True Test Set (20%) ---
# We split the CLEAN data, not the raw mega_data
trainIndex <- createDataPartition(rf_data$Age, p = .8, list = FALSE, times = 1)
data_train <- rf_data[ trainIndex,]
data_test  <- rf_data[-trainIndex,]

# --- 3. Oversample ONLY Training Data ---
grp_36 <- data_train[data_train$Age == "36+", ]
grp_22 <- data_train[data_train$Age == "22-25", ]
grp_31 <- data_train[data_train$Age == "31-35", ]

# Duplicate to balance (Target ~370)
# Note: We don't touch the "26-30" group as it's already large
boost_36 <- grp_36[rep(1:nrow(grp_36), 40), ]
boost_22 <- grp_22[rep(1:nrow(grp_22), 2), ]
boost_31 <- grp_31[rep(1:nrow(grp_31), 1), ]

# Combine
train_final <- rbind(data_train, boost_36, boost_22, boost_31)

# --- 4. Train Model ---
print("Training Balanced Random Forest...")
rf_final <- randomForest(Age ~ ., data = train_final, ntree=500, mtry=15)

# --- 5. Predict on Clean Test Set ---
predictions <- predict(rf_final, newdata = data_test)

# --- 6. Results ---
conf_matrix <- confusionMatrix(predictions, data_test$Age)
print(conf_matrix)
```

```{r rf_final_merged, echo=TRUE}
library(caret)
library(randomForest)

set.seed(999)

# 1. Prepare Data (Structure + Function)
# We start with the 120-feature "mega_data" you loaded earlier
rf_data <- mega_data[, c("Age", grep("PC", colnames(mega_data), value=TRUE))]

# 2. Merge "36+" into "31-35" -> Create "31+" Group
# This fixes the sample size issue
rf_data$Age <- as.character(rf_data$Age)
rf_data$Age[rf_data$Age == "36+"] <- "31+"
rf_data$Age[rf_data$Age == "31-35"] <- "31+"
rf_data$Age <- as.factor(rf_data$Age)

print("New Class Sizes:")
print(table(rf_data$Age))

# 3. Create Test/Train Split (80/20)
trainIndex <- createDataPartition(rf_data$Age, p = .8, list = FALSE)
data_train <- rf_data[ trainIndex,]
data_test  <- rf_data[-trainIndex,]

# 4. Oversample ONLY Training Data (To balance the 3 groups)
# Target size: ~370 (Size of the "26-30" group)
grp_22 <- data_train[data_train$Age == "22-25", ]
grp_31 <- data_train[data_train$Age == "31+", ]

# Replicate to match the size of the "26-30" group
# 22-25 needs ~2x
boost_22 <- grp_22[rep(1:nrow(grp_22), 2), ]
# 31+ is now big enough! It might just need a tiny boost (1.2x) or none. 
# Let's just double it to be safe and give the model strong signal.
boost_31 <- grp_31[rep(1:nrow(grp_31), 1), ] 

train_final <- rbind(data_train, boost_22, boost_31)

# 5. Train & Test
print("Training Final Optimized Model...")
rf_merged <- randomForest(Age ~ ., data = train_final, ntree=500, mtry=20)
predictions <- predict(rf_merged, newdata = data_test)

# 6. Final Score
conf_matrix <- confusionMatrix(predictions, data_test$Age)
print(conf_matrix)
```

```{r graph_theory, echo=TRUE}
library(ggplot2)
library(dplyr)
library(R.matlab)

# 1. Load Raw Structural Matrices (The Physical Wiring)
raw_data <- readMat("Data/SC/HCP_cortical_DesikanAtlas_SC.mat")
connectivity <- raw_data$hcp.sc.count # 68 x 68 x 1065 Array

# 2. Calculate "Global Strength" for each subject
# This is simply the sum of all white matter fibers in the brain
# A higher number = A more densely connected brain
global_strength <- apply(connectivity, 3, sum)

# 3. Add to your existing dataframe
# We use 'mega_data' from the previous step
# (Make sure to match Subject IDs if you want to be perfectly safe, 
# but usually the order is consistent. We'll assume consistency for this quick check)
mega_data$Brain_Strength <- global_strength[1:nrow(mega_data)] 

# 4. Visualization: Does Brain Strength drop with Age?
# We expect the "31+" box to be lower than the "22-25" box
ggplot(mega_data, aes(x = Age, y = Brain_Strength, fill = Age)) +
  geom_boxplot() +
  geom_jitter(alpha=0.1) + # Show individual dots
  theme_minimal() +
  labs(title = "Neuro-Scientific Validation: Brain Connectivity vs Age",
       subtitle = "Older brains tend to have fewer total structural connections (lower density)",
       y = "Total Streamline Count (Log Scale)") +
  scale_y_log10() # Log scale helps visualization
```

```{r cognition_correlation_final, echo=TRUE, message=FALSE, warning=FALSE}
library(ggplot2)
library(dplyr)
library(ggpubr)
library(R.matlab)

# --- 1. Get Brain Strength (Fresh Load) ---
# We load the raw matrix directly to be safe
raw_sc <- readMat("Data/SC/HCP_cortical_DesikanAtlas_SC.mat")
# Calculate sum of all connections for each subject
brain_strength <- apply(raw_sc$hcp.sc.count, 3, sum)
# Get IDs
subjects_sc <- as.vector(raw_sc$all.id)

# Create a clean table just for Strength
df_strength <- data.frame(Subject = as.character(subjects_sc), 
                          Brain_Strength = brain_strength)

# --- 2. Get Intelligence Scores (Fresh Load) ---
# We load Table 1 again to ensure we have the PMAT variable
df_traits <- read.csv("Data/traits/table1_hcp.csv", stringsAsFactors=FALSE)
# Select only what we need
df_cognition <- df_traits %>% 
  select(Subject, PMAT24_A_CR) %>%
  mutate(Subject = as.character(Subject)) # Ensure ID is text to match

# --- 3. Merge ---
# Inner Join keeps only people with BOTH valid brain data and IQ scores
plot_data <- inner_join(df_strength, df_cognition, by = "Subject")

# Remove any missing IQ scores (NAs)
plot_data <- plot_data %>% filter(!is.na(PMAT24_A_CR))

# --- 4. The Plot ---
# Calculate correlation for the title
res <- cor.test(plot_data$Brain_Strength, plot_data$PMAT24_A_CR)
r_val <- round(res$estimate, 2)
p_val <- signif(res$p.value, 3)

ggplot(plot_data, aes(x = Brain_Strength, y = PMAT24_A_CR)) +
  geom_point(alpha = 0.5, color = "#2c3e50") + # Dark blue dots
  geom_smooth(method = "lm", color = "#e74c3c", fill = "#e74c3c", alpha = 0.2) + # Red trend line
  theme_minimal() +
  labs(title = "Neuro-Cognitive Link: Wiring vs. Intelligence",
       subtitle = paste0("Correlation: R = ", r_val, " (p = ", p_val, ")"),
       x = "Total Structural Connectivity (Global Strength)",
       y = "Fluid Intelligence (PMAT24 Score)")
```
=======


```{r}
# Ensure Gender is a factor for plotting
full_data$Gender <- as.factor(full_data$Gender)

ggplot(full_data, aes(x = PC1, y = PC2, color = Gender, fill = Gender)) +
  geom_point(alpha = 0.4, size = 1.5) + 
  # Add 95% confidence ellipses
  stat_ellipse(geom = "polygon", alpha = 0.2, level = 0.95) +
  scale_color_manual(values = c("M" = "#1f77b4", "F" = "#d62728")) +
  scale_fill_manual(values = c("M" = "#1f77b4", "F" = "#d62728")) +
  theme_bw() +
  labs(
    title = "Structural Connectome Separation by Gender",
    subtitle = "PC1 vs PC2 with 95% Confidence Regions",
    x = "Principal Component 1",
    y = "Principal Component 2"
  )
```



```{r}
ggplot(full_data, aes(x = Gender, y = Strength_Unadj, fill = Gender)) +
  geom_boxplot(alpha = 0.7) +
  geom_jitter(width = 0.2, alpha = 0.1) +
  labs(title = "Sanity Check: Sexual Dimorphism in Grip Strength",
       y = "Grip Strength (lbs)") +
  theme_minimal()
```





```{r}
# 2. Brain Structure PCA Plot
ggplot(full_data, aes(x = PC1, y = PC2, color = Gender)) +
  geom_point(alpha = 0.5) +
  stat_ellipse(level = 0.95, size = 1) + # Adds 95% confidence circles
  scale_color_manual(values = c("M" = "#1f77b4", "F" = "#d62728")) +
  labs(title = "Brain Structural Connectome: Male vs Female",
       subtitle = "Do the ellipses separate?",
       x = "PC1 (Main Variation)", y = "PC2 (Secondary Variation)") +
  theme_minimal()
```



```{r}
# Create a storage for results
results <- data.frame(PC = character(), P_Value = numeric(), Cohen_D = numeric(), stringsAsFactors = FALSE)

# Loop through first 10 PCs (or all 60 if you want)
for(i in 1:60) {
  pc_name <- paste0("PC", i)
  
  # Run t-test
  test <- t.test(full_data[[pc_name]] ~ full_data$Gender)
  
  # Calculate Cohen's d (Effect Size) - simplified
  mean_diff <- diff(test$estimate)
  pooled_sd <- sd(full_data[[pc_name]])
  cohen_d <- mean_diff / pooled_sd
  
  # Store
  results[i, ] <- c(pc_name, test$p.value, cohen_d)
}

# Adjust p-values (False Discovery Rate)
results$P_Adj <- p.adjust(as.numeric(results$P_Value), method = "fdr")
results$Significant <- results$P_Adj < 0.05

print(results)
```






```{r}
# Install packages if you don't have them
# install.packages("randomForest")
# install.packages("caret") 

library(randomForest)
library(caret)
library(ggplot2)

# Data Preparation
# We need a clean dataframe with just the Predictors (X) and Target (Y)
# Target: Gender (Factor)
# Predictors: PC1-PC60 (Structure) AND Func_PC1-Func_PC60 (Functional)
# Control: Age, Brain Volume (FS_BrainSeg_Vol)

rf_data <- full_data %>%
  dplyr::select(Gender, starts_with("PC"), starts_with("Func_PC"), FS_BrainSeg_Vol) %>%
  mutate(Gender = as.factor(Gender)) %>%
  na.omit()

# Split Data (
set.seed(123) # For reproducibility
trainIndex <- createDataPartition(rf_data$Gender, p = .7, list = FALSE)
train_set <- rf_data[trainIndex, ]
test_set  <- rf_data[-trainIndex, ]

print(paste("Training on", nrow(train_set), "subjects."))

# Train the Random Forest 
rf_model <- randomForest(Gender ~ ., data = train_set, ntree = 500, importance = TRUE)

# Evaluate Performance
predictions <- predict(rf_model, test_set)
conf_matrix <- confusionMatrix(predictions, test_set$Gender)
print(conf_matrix)

# Feature Importance
# Extract importance scores
importance_df <- as.data.frame(importance(rf_model))
importance_df$Feature <- rownames(importance_df)

# Get the Top 10 most important features
top_features <- importance_df %>%
  arrange(desc(MeanDecreaseGini)) %>%
  head(10)

# Plot the Biomarkers
ggplot(top_features, aes(x = reorder(Feature, MeanDecreaseGini), y = MeanDecreaseGini, fill = Feature)) +
  geom_bar(stat = "identity") +
  coord_flip() + # Horizontal bars are easier to read
  labs(title = "Top 10 Biomarkers for Sexual Dimorphism",
       subtitle = "Which brain features best distinguish Male from Female?",
       x = "Feature Name",
       y = "Importance (Mean Decrease Gini)") +
  theme_minimal() +
  theme(legend.position = "none")
```





```{r}
# Load necessary libraries
library(xgboost)
library(caret)
library(dplyr)

select <- dplyr::select
# Ensure no missing values exist
clean_data <- full_data %>%
  select(Gender, starts_with("PC"), starts_with("Func_PC"), FS_BrainSeg_Vol) %>%
  na.omit()

# Prepare the Target variable
# Convert Gender to 0 and 1
data_y <- as.numeric(as.factor(clean_data$Gender)) - 1

# Prepare the Predictor Matrix
data_x <- clean_data %>%
  select(-Gender) %>%
  as.matrix()

# Split Data into Training and Testing
# We use 70 percent for training and 30 percent for testing
set.seed(123)
train_index <- createDataPartition(data_y, p = 0.7, list = FALSE)

# Create the DMatrix objects for XGBoost
dtrain <- xgb.DMatrix(data = data_x[train_index, ], label = data_y[train_index])
dtest  <- xgb.DMatrix(data = data_x[-train_index, ], label = data_y[-train_index])

# Set parameters
params <- list(
  objective = "binary:logistic",
  eta = 0.05,
  max_depth = 4,
  eval_metric = "error"
)

# Train the Model
model_xgb <- xgboost(
  params = params,
  data = dtrain,
  nrounds = 500,
  verbose = 0
)

# Make Predictions on the Test Set
# XGBoost outputs probabilities (0 to 1)
pred_probs <- predict(model_xgb, dtest)

# Convert Probabilities to Class Labels
# If probability is greater than 0.5 we classify as 1 (Male)
pred_labels <- ifelse(pred_probs > 0.5, 1, 0)

# Convert vectors to factors for the Confusion Matrix
# We map 0 and 1 back to the original Gender levels (F and M)
actual_labels <- factor(data_y[-train_index], levels = c(0, 1), labels = c("F", "M"))
predicted_labels <- factor(pred_labels, levels = c(0, 1), labels = c("F", "M"))

# Generate the Confusion Matrix and Statistics
conf_matrix <- confusionMatrix(predicted_labels, actual_labels)

# Print the Results
print(conf_matrix)

# Print specific accuracy to ensure it is clear
print(paste("Accuracy:", round(conf_matrix$overall["Accuracy"], 4)))
```






>>>>>>> Stashed changes

## Introduction

The human brain structural connectome,  defined here as the collection of white matter fiber tracts connecting different regions of the brain [see @park2013structural],[@fornito2013graph],[@craddock2013imaging] and [@Jones2013], plays a crucial role in how the brain responds to everyday tasks and life's challenges.  There has been a huge interest in studying connectomes and understanding how they vary for individuals in different groups according to traits and substance exposures. Such studies have typically focused on functional connectomes instead of structural connectomes [see @park2013structural], [@finn2015functional], [@bjork2014effects], [@smith2015positive], and [@price2012review], due to the difficulty of recovering reliable structural connectomes [@maier2016tractography], [@reveley2015superficial]. If you are interested to learn more, you can download and read these papers. 

Recent advances in noninvasive brain imaging and preprocessing have produced huge brain imaging datasets (e.g., the Human Connectome Project [@van2013wu] and the UK Biobank [@miller2016multimodal]) along with sophisticated tools  to routinely extract brain structural connectomes for different individuals. We will mainly focus on the Humcan Connectome Project. You can get an overview of this project from (https://www.humanconnectome.org/). You will find that there are different studies: Young Adult HCP, Lifespan HCP and so on. We will focus on the Yong Adult HCP. **For students who decided to choose this project, you need to register an account at (https://db.humanconnectome.org/) to agree with their data usage terms.**

Relying on high quality imaging data and many different traits for a large number of study participants obtained in the Human Connectome Project (HCP) we can extract reliable structural connectomes and different traits and substance exposures measures. The goal of this project is to analyze relationships between them. 

**Human Connectome Project (HCP)**. The HCP aims to characterize human brain connectivity in about $1,200$ healthy adults and to enable detailed comparisons between brain circuits, behavior and genetics at the level of individual subjects. Customized scanners were used to produce high-quality and consistent data to measure brain connectivity. The latest release in 2017, containing various traits, structural MRI (sMRI) and diffusion MRI (dMRI)  data for $1065$ healthy adults, can be easily accessed through ConnectomeDB. The rich trait data, high-resolution dMRI and sMRI make it an ideal data set for studying relationships between connectomes and human traits.

(Some background on diffusion MRI, not necessary to understand everything) A full diffusion MRI session in HCP includes 6 runs (each approximately 10 minutes),
representing 3 different gradient tables, with each table acquired once with right-to-left and left-to-right phase encoding polarities, respectively. Each gradient table includes approximately $90$
diffusion weighting directions plus 6 $b_0$ acquisitions interspersed throughout each
run.  Within each run, there are  three shells of $b=1000, 2000$, and $3000$ s/mm$^2$ interspersed
with an approximately equal number of acquisitions on each shell.   The directions were optimized so that
every subset of the first $N$ directions is also isotropic. The scan was done by using the Spin-echo EPI sequence on a 3 Tesla customized Connectome Scanner. See [@VanEssen20122222] for more details about the data acquisition of HPC. Such settings give the final acquired image with isotropic voxel size of $1.25$ mm, and $270$ diffusion weighted scans distributed equally over $3$ shells.

**Structural Connectome Reconstruction**. The first step toward structrual connectome reconstruction is to rebuild fibers (called tractography) from dMRI measures. We use a method in [@Girard2014266], and [@maier2016tractography]  to generate the whole-brain tractography data set of  each subject  for all data sets. The method borrows anatomical information from high-resolution T1-weighted imaging to reduce bias in reconstruction of tractography. Also the parameters are selected based on evaluation of various global connectivity metrics in \citep{Girard2014266}. In the generated tractography data, each streamline has a step size of 0.2 mm. On average, $10^5$ voxels were identified as the seeding region (white matter and gray matter interface region) for each individual in the HCP data set (with isotropic voxel size of 1.25 mm).  For each seeding voxel, we initialized $16$ fibers to generate about $10^6$ fibers for each subject.  

We then segement the whole brain into a few regions of interest so that we can extract a connectivity matrix for each subject to describe which brain regions are connected. Usually, this step is done by utlizing some well known atlas/template predefined by brain anatomist. One of the most common one is called Desikan-Killiany atlas. The Desikan-Killiany atlas parcellate the brain into 68 cortical surface regions with 34 nodes in each hemisphere. Freesurfer software can be used to perform brain registration and parcellation. Figure 1 column (a) illustrates the Desikan-Killiany parcellation and a reconstructed tractography data after subsampling. 

![**Figure 1** - Pipeline of the preprocessing steps to extract weighted networks from the dMRI and sMRI image data. (a) Desikan-Killiany parcellation and the tractography data for an individual's brain; (b) extraction of streamlines between two ROIs; (c) feature extraction from each connection and (d) extracted weighted networks. ](pipeline_illustration_modified1.png)

With the parcellation of an individual brain, we extract a set of weighted matrices to represent the brain's structural connectome. For each pair of ROIs, we first extract fibers connecting them. Figure 1 column (d) shows one example of the fiber curves. We then extract different summaries/features to characterize each fiber bundle (in a connection). In this project, you will only get fiber count matrices. 

**Traits**. You can find many covariates in the folder of `Data/traits` (i.e., `table1_hcp.csv` and `table2_hcp.csv`) and data dictionary in `HCP_S1200_DataDictionary_Sept_18_2017.xls`. A subfolder named `175traits` contains 175 traits that I previously identified. These traits measure a person's cognition, substance use, psychiatric and life function, sensory, emotion, health and family history. 

**Structural and Functional Connectomes**. In the folder `SC` and `FC` you can find Structural Connectomes and Functional Connectomes for about 1065 subjects, respectively.  

**Structural Connectome Dimention Reduction** Let's assume we extract M different features from each connection. By doing this, we have a $68 \times 68 \times M$ matrix (for the Desikan Parcellation. You might see $87\times 87$ or other dimensions if a different parcellation is used to construct the SC or FC) representing each subject's structural connectome. If you calculate the number of variables inside such tensor, it should be $(68*67/2)*M = 2278*M$ (because of the symmetry of the matrix). If $M = 10$, you have about 22k unknown variables. Usually, with such large unknown variables and such small sample size, we are facing challenges in modeling. For example, a simple linear regression will not work (why?). Let's say,  we are interested in using these connectivity features to predict a score measured from a subject, which model can we use?

A few ways to get around the high dimensional issue include: (1) if you want to use a regression method, you can perform a penalize regression, for example, rigid regression or LASSO; (2) reduce the dimension of the connectivity matrix. There are many ways for (2) - for example, you can treat the matrix as a set of network and extract summary statistics from these networks. Depending on how many of summaries you extract, you will have different low dimensional representation of the connectome. Another way is to vectorize the data and perform PCA analysis.  However, vectorizing the tensor would discard valuable information about the network structure. Here we take a different approach - we treat this set of networks for all subjects as a high-dimensional tensor and perform a tenor PCA. 

The tensor PCA (TNPCA) approach approximates the brain tensor network using $K$ components, with the components ordered to have decreasing impact similarly to common practice in PCA.  Individuals are assigned a brain connectome PC score for each of the $K$ components, measuring the extent to which their brain tensor network expresses that particular tensor network (TN) component.  Each TN component is itself a tensor network, but one having a very simple rank one structure depending on scores for each brain ROI.  By using TN-PCA, we can replace the high dimensional tensor network summarizing an individual's brain connectome with a $K$-dimensional vector of brain PC scores; these scores can be used for visualizing variation among individuals in their brain connectomes and in statistical analyses studying relationships between connectomes and traits. You can find matlab code for performing TNPCA here: https://github.com/zhengwu/TensorNetwork_PCA. By setting `K=60`, I calculated the TN-PCA coefficient scores for each subject and you can find these scores in the folder `TNPCA_Result` if you want to utilize such data in your project. 

**Network data embedding**. Loosely, there are two types of embedding: 1. you can embed the whole network into a vector in a low-dimensional space; and 2. you can embed each network node into a vector space. In the later case, the spatial distribution of nodes in the embedding space would represent the relationship among them, e.g., connected nodes will stay close to each other. There is a large body of literature for this thread (e.g, embedding of large scale graphs and so on). A very recent paper is by Arroyo et al. [@arroyo2019inference] that you can read and implement for this project (he has R and Python code, check his website: https://jesus-arroyo.github.io/papers/)

**Load data**. 
The matlab code "load_data.m" show you how to load data to matlab. You can  `R.matlab` package in R to load `.mat` file to R. 

Final project questions (suggested)
===
For the final project, your tasks can be simply summarized as follows:

- merge all data into one data frame/tibble (if you use the TNPCA scores, also include this in your data frame)

- As an alternative, you can build different relationale databases for all modalities and practice SQL and `dbplyr` with such data.  

- Conduct EDA to the data, including data visualziation, distribution spection and so on. Summarize some interesting results into your report. 

- Plot some interesting PC scores v.s. traits (See figure 4 panel c). 

- study the relationship between traits and structural connectome and/or functional connectome

- try to study some neuroscience and explain your results. 

- write your report and presentation.


Merge data or data management
===
For each subject, you have network data, PCA scores, and covariates. Start to think about how to store them in an efficient way to faciliate your modeling and visualization.

Plots or data visulization 
===
Before writing down any models, EDA is encouraged. Visualize your data and try to see whether there are interesting patterns/trends inside. Here we are interested in exploring the relationship between traits and brain connectomes. I will suggest to 1) reduce the dimension of the network data into a vector (e.g., with dimenion of 3 so you can plot them), 2) visulize the embedded networks and explore their relationship with some traits (e.g., cognition or motion). 

What you can do is to sort all subjects according to some trait, take out the first N and last N subjects (e.g., N=50,100,200), plot their first 3 PC scores, and color each subject with the trait score. Choose some interesting ones to display and explain their meaning. 

Study the relationship
===
To study the relationship between traits and connectomes, you can do: 1. hypothesis tests and 2. prediction and other interesting statistical analyses that you know well (or you want to study).  

Note that some of the traits are discrete and some of them are continuous. You might want to use different models for different types of covariates. 

For hypothesis tests, what you can do is to choose a trait, sort the subject, get N subjects with high trait values and get M subjects with low trait values, assign them to high and low classes. Next, you test the mean difference between their connectomes and/or PC scores.

If you only test the first PC scores, you can use t-test. However, if you want to test mean difference for multivariate data (with dim>1), you need to do more research (e.g., which method will work in this case). Try to find some R/Matlab package can do this type of hypothese testing. 

For prediction analysis, I suggest you try different models, e.g., linear regression, rigid regression, SVM, random forest and so on. Choose some traits that are interesting to you, and try use different models and evaluate whether brain connectomes can predict traits. That is whether you brain connectivity can predict your cognition, emotion and so on. 


**References**