---
title: "pca-vae"
author: "Sean Shen"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(R.matlab)
library(tidyverse)
```


```{r torch setup}
if (!requireNamespace("torch", quietly = TRUE)) {
  install.packages("torch")
}

# 2. Load the library
library(torch)

# 3. Check if the C++ backend (Lantern) is installed
# If not, install it automatically.
if (!torch::torch_is_installed()) {
  torch::install_torch()
}

# might need vc_redist.x64.exe on windows
```


# Principal Component Analysis


## Structural Connectome
```{r struct}
# Load Raw Structural Data
# This is the 68x68x1065 Tensor
struct_raw <- readMat("Data/SC/HCP_cortical_DesikanAtlas_SC.mat")
sc_tensor  <- struct_raw$hcp.sc.count
sc_ids     <- as.vector(struct_raw$all.id)

n_subs <- length(sc_ids)
# There are 68x67/2 = 2278 unique connections in a symmetric matrix
n_feats <- 68 * 67 / 2 

# Flatten the 3D Tensor into a 2D Matrix (Subjects x Connections)
# We only take the upper triangle to avoid duplicates
sc_flat <- matrix(NA, nrow = n_subs, ncol = n_feats)

print("Flattening Structural Data...")
for (i in 1:n_subs) {
  # Extract single subject matrix
  mat <- sc_tensor[,,i]
  # Flatten upper triangle
  sc_flat[i, ] <- mat[upper.tri(mat)]
}


# Remove Zero Variance Columns
# We calculate variance for every column
col_vars <- apply(sc_flat, 2, var)

# We identify columns where variance is 0 (Constant columns)
const_cols <- which(col_vars == 0)

print(paste("Removing", length(const_cols), "constant columns..."))

# We subset the matrix to keep only columns with variance > 0
if (length(const_cols) > 0) {
  sc_flat_clean <- sc_flat[, -const_cols]
} else {
  sc_flat_clean <- sc_flat
}

# Run Standard PCA
# rank. = 60 keeps only the top 60 components to match TNPCA
print("Running Structural PCA...")
sc_pca <- prcomp(sc_flat_clean, center = TRUE, scale. = TRUE, rank. = 60)

# Create Dataframe
# We name them "Raw_Struct_PC" to distinguish from the TNPCA "Struct_PC"
my_struct_df <- data.frame(Subject = sc_ids, sc_pca$x)
colnames(my_struct_df)[2:61] <- paste0("Raw_Struct_PC", 1:60)
```

## Functional Connectome

```{r func}
library(R.matlab)
library(dplyr)

# --- Load Raw Functional Data ---
func_raw <- readMat("Data/FC/HCP_cortical_DesikanAtlas_FC.mat")
fc_list  <- func_raw$hcp.cortical.fc
fc_ids   <- as.vector(func_raw$subj.list)

n_subs_fc <- length(fc_ids)
n_feats <- 68 * 67 / 2 

# Initialize with NA (So bad subjects just stay as NA)
fc_flat <- matrix(NA, nrow = n_subs_fc, ncol = n_feats)

print("Flattening Functional Data (with Error Checking)...")
for (i in 1:n_subs_fc) {
  # Extract matrix
  mat <- fc_list[[i]]
  if (is.list(mat)) { mat <- mat[[1]] } 
  
  # If matrix is actually 68x68
  if (all(dim(mat) == c(68, 68))) {
    fc_flat[i, ] <- mat[upper.tri(mat)]
  } else {
    # If bad (0x0), we do nothing. The row stays as NA.
    print(paste("Skipping Subject Index:", i, "- Empty/Bad Matrix"))
  }
}

# --- Drop Bad Subjects ---
# We verify which rows are still NA and remove them
bad_rows <- which(is.na(fc_flat[,1]))
if (length(bad_rows) > 0) {
  print(paste("Removing", length(bad_rows), "failed subjects."))
  fc_flat_clean <- fc_flat[-bad_rows, ]
  fc_ids_clean  <- fc_ids[-bad_rows]
} else {
  fc_flat_clean <- fc_flat
  fc_ids_clean  <- fc_ids
}

# --- Remove Zero Variance Columns ---
col_vars_fc <- apply(fc_flat_clean, 2, var)
const_cols_fc <- which(col_vars_fc == 0)

if (length(const_cols_fc) > 0) {
  fc_flat_final <- fc_flat_clean[, -const_cols_fc]
} else {
  fc_flat_final <- fc_flat_clean
}

# --- Run PCA ---
print("Running Functional PCA...")
fc_pca <- prcomp(fc_flat_final, center = TRUE, scale. = TRUE, rank. = 60)

# --- Create Dataframe ---
my_func_df <- data.frame(Subject = fc_ids_clean, fc_pca$x)
colnames(my_func_df)[2:61] <- paste0("Raw_Func_PC", 1:60)

print("Functional PCA Complete.")
```



```{r}
write.csv(my_struct_df, "Data/raw_struct_pca.csv")
write.csv(my_func_df, "Data/raw_func_pca.csv")
```


# Variational Autoencoder

## Structural Connectome
```{r}
library(torch)
library(dplyr)

# Prepare Data
# x_mat <- scale(sc_flat_clean)
# x_tensor <- torch_tensor(x_mat, dtype = torch_float())


x_mat_log <- log1p(sc_flat_clean) # log(x + 1)

# THEN normalize to 0-1 range (Min-Max) for stability
max_val <- max(x_mat_log)
x_mat_final <- x_mat_log / max_val

# Convert to Tensor
x_tensor <- torch_tensor(x_mat_final, dtype = torch_float())

# Split Data (80% Train, 20% Test)
set.seed(123)
torch_manual_seed(123)
total_rows <- nrow(x_tensor)
#sample_idx <- sample(1:nrow(x_tensor), size = 0.8 * nrow(x_tensor))
train_indices <- sample(1:total_rows, size = 0.8 * total_rows)
test_indices  <- setdiff(1:total_rows, train_indices)
train_tensor <- x_tensor[train_indices, ]
test_tensor  <- x_tensor[test_indices, ] # The held-out set for testing

input_dim <- ncol(x_mat)
latent_dim <- 60  
hidden_dim <- 256

# Define VAE Architecture
vae_module <- nn_module(
  "VAE",
  
  initialize = function(input_dim, hidden_dim, latent_dim) {
    self$fc1 <- nn_linear(input_dim, hidden_dim)
    self$drop <- nn_dropout(p = 0.2)
    
    self$fc_mu <- nn_linear(hidden_dim, latent_dim)
    self$fc_logvar <- nn_linear(hidden_dim, latent_dim)
    self$fc_d1 <- nn_linear(latent_dim, hidden_dim)
    self$fc_d2 <- nn_linear(hidden_dim, input_dim)
  },
  
  encode = function(x) {
    h <- torch_relu(self$fc1(x))
    h <- self$drop(h)
    mu <- self$fc_mu(h)
    logvar <- self$fc_logvar(h)
    return(list(mu, logvar))
  },
  
  reparameterize = function(mu, logvar) {
    std <- torch_exp(0.5 * logvar)
    eps <- torch_randn_like(std)
    return(mu + eps * std)
  },
  
  decode = function(z) {
    h <- torch_relu(self$fc_d1(z))
    h <- self$drop(h)
    return(self$fc_d2(h))
  },
  
  forward = function(x) {
    enc_out <- self$encode(x)
    mu <- enc_out[[1]]
    logvar <- enc_out[[2]]
    z <- self$reparameterize(mu, logvar)
    recon_x <- self$decode(z)
    return(list(recon_x, mu, logvar, z))
  }
)

# Initialize Model & Optimizer
model <- vae_module(input_dim, hidden_dim, latent_dim)
optimizer <- optim_adam(model$parameters, lr = 0.001, weight_decay = 0.0005)

# Loss Function
vae_loss <- function(recon_x, x, mu, logvar) {
  MSE <- nnf_mse_loss(recon_x, x, reduction = "sum")
  KLD <- -0.5 * torch_sum(1 + logvar - torch_pow(mu, 2) - torch_exp(logvar))
  return(MSE + KLD)
}

# Training Loop
num_epochs <- 200
batch_size <- 64
num_train <- nrow(train_tensor)
num_test <- nrow(test_tensor)

train_loss_norm_history <- numeric(num_epochs)
test_loss_norm_history  <- numeric(num_epochs)

print("Training VAE with Normalized Loss Tracking...")

for (epoch in 1:num_epochs) {
  
  # Training Phase
  model$train()
  indices <- sample(1:num_train)
  train_loss_sum <- 0
  train_var_sum  <- 0
  
  for (i in seq(1, num_train, by = batch_size)) {
    end_idx <- min(i + batch_size - 1, num_train)
    batch_idx <- indices[i:end_idx]
    batch_x <- train_tensor[batch_idx, ]
    
    optimizer$zero_grad()
    
    output <- model(batch_x)
    # Calculate raw MSE for the batch (reconstruction only)
    batch_mse <- nnf_mse_loss(output[[1]], batch_x, reduction = "sum")
    
    # Calculate Loss (MSE + KLD) for backprop
    loss <- vae_loss(output[[1]], batch_x, output[[2]], output[[3]])
    loss$backward()
    
    train_loss_sum <- train_loss_sum + batch_mse$item() # Track raw MSE
    # Calculate batch variance sum (sum of squares since mean approx 0.5)
    # Or just use the global variance for stability
    optimizer$step()
  }
  
  # Validation Phase
  if (epoch %% 1 == 0) { # Check every epoch for smooth plot
    model$eval()
    test_loss_sum <- 0
    
    with_no_grad({
      output_test <- model(test_tensor)
      # Track pure reconstruction error (MSE) for metric
      batch_test_mse <- nnf_mse_loss(output_test[[1]], test_tensor, reduction = "sum")
      test_loss_sum <- batch_test_mse$item()
    })
    
    # CALCULATE NORMALIZED LOSS (Unexplained Variance)
    # Loss / (N * Variance)
    # Since data is MinMax scaled, variance is roughly 0.08 (approx)
    # Better: Divide MSE by Total Sum of Squares (TSS) of the data subset
    
    # Calculate Total Sum of Squares for Train and Test sets
    train_tss <- sum((as.matrix(train_tensor) - mean(as.matrix(train_tensor)))^2)
    test_tss  <- sum((as.matrix(test_tensor)  - mean(as.matrix(test_tensor)))^2)
    
    train_norm_loss <- train_loss_sum / train_tss
    test_norm_loss  <- test_loss_sum  / test_tss
    
    train_loss_norm_history[epoch] <- train_norm_loss
    test_loss_norm_history[epoch]  <- test_norm_loss
    
    if (epoch %% 10 == 0) {
      cat(sprintf("Epoch %d | Norm Train Loss: %.4f | Norm Test Loss: %.4f\n", 
                  epoch, train_norm_loss, test_norm_loss))
    }
  }
}

# Plot Normalized Loss
history_norm_df <- data.frame(
  Epoch = 1:num_epochs,
  Train = train_loss_norm_history,
  Test  = test_loss_norm_history
) %>%
  pivot_longer(cols = c("Train", "Test"), names_to = "Type", values_to = "NormalizedLoss")

ggplot(history_norm_df, aes(x = Epoch, y = NormalizedLoss, color = Type)) +
  geom_line(size = 1) +
  scale_y_continuous(limits = c(0, 1)) + # Force 0-1 scale
  scale_color_manual(values = c("Train" = "#1f77b4", "Test" = "#d62728")) +
  labs(title = "VAE Normalized Learning Curve",
       subtitle = "Proportion of Unexplained Variance",
       y = "Normalized Loss (1 - R^2)") +
  theme_minimal()

# Extract Latent Variables (Using ALL data for the final features)
# We switch back to eval mode to get deterministic features
model$eval()
final_out <- model(x_tensor)
vae_scores <- as.matrix(final_out[[2]]) # Extract Mu

# Create Dataframe
vae_struct_df <- data.frame(Subject = sc_ids, vae_scores)
colnames(vae_struct_df)[2:(latent_dim+1)] <- paste0("VAE_Struct_LD", 1:latent_dim)

```





```{r}
# Visualization of the "Magic" Fix
par(mfrow=c(1,2)) # Side-by-side plots

# 1. The Nightmare (Raw Data)
# Shows almost everything at 0, with invisible outliers at 5000
hist(sc_flat_clean[sample(length(sc_flat_clean), 10000)], 
     main = "Raw Counts (The Problem)", 
     xlab = "Fiber Count", col = "red", breaks = 50)

# 2. The Solution (Log Data)
# Shows a beautiful, learnable distribution
hist(log1p(sc_flat_clean[sample(length(sc_flat_clean), 10000)]), 
     main = "Log Transformed (The Fix)", 
     xlab = "Log(Fiber Count)", col = "blue", breaks = 50)
```



## Functional Connectome


```{r}
library(torch)
library(dplyr)
library(ggplot2)
library(tidyr)

# GIGACHAD REPRODUCIBILITY BLOCK
set.seed(123)
torch_manual_seed(123)

# 1. Prepare Data (Log Transform + MinMax)
# Note: fc_flat_final is your raw functional connectivity matrix
# We apply log1p to handle any high-value outliers in functional strength
x_mat_func_log <- log1p(fc_flat_final)
max_val_func   <- max(x_mat_func_log)
x_mat_func_final <- x_mat_func_log / max_val_func

# Convert to Tensor
x_tensor_func <- torch_tensor(x_mat_func_final, dtype = torch_float())

# 2. Split Data (Using robust setdiff logic)
total_rows_func <- nrow(x_tensor_func)
train_indices_func <- sample(1:total_rows_func, size = 0.8 * total_rows_func)
test_indices_func  <- setdiff(1:total_rows_func, train_indices_func)

train_tensor_func <- x_tensor_func[train_indices_func, ]
test_tensor_func  <- x_tensor_func[test_indices_func, ]

# Dimensions
input_dim  <- ncol(x_mat_func_final)
latent_dim <- 60
hidden_dim <- 256

# 3. Define VAE Architecture (With Dropout)
vae_module_func <- nn_module(
  "VAE_Func",
  
  initialize = function(input_dim, hidden_dim, latent_dim) {
    self$fc1 <- nn_linear(input_dim, hidden_dim)
    self$drop <- nn_dropout(p = 0.2) # Regularization
    
    self$fc_mu <- nn_linear(hidden_dim, latent_dim)
    self$fc_logvar <- nn_linear(hidden_dim, latent_dim)
    
    self$fc_d1 <- nn_linear(latent_dim, hidden_dim)
    self$fc_d2 <- nn_linear(hidden_dim, input_dim)
  },
  
  encode = function(x) {
    h <- torch_relu(self$fc1(x))
    h <- self$drop(h)
    mu <- self$fc_mu(h)
    logvar <- self$fc_logvar(h)
    return(list(mu, logvar))
  },
  
  reparameterize = function(mu, logvar) {
    std <- torch_exp(0.5 * logvar)
    eps <- torch_randn_like(std)
    return(mu + eps * std)
  },
  
  decode = function(z) {
    h <- torch_relu(self$fc_d1(z))
    h <- self$drop(h)
    return(self$fc_d2(h))
  },
  
  forward = function(x) {
    enc_out <- self$encode(x)
    mu <- enc_out[[1]]
    logvar <- enc_out[[2]]
    z <- self$reparameterize(mu, logvar)
    recon_x <- self$decode(z)
    return(list(recon_x, mu, logvar, z))
  }
)

# Initialize
model_func <- vae_module_func(input_dim, hidden_dim, latent_dim)
optimizer_func <- optim_adam(model_func$parameters, lr = 0.001, weight_decay = 0.0005)

# Loss Function
vae_loss_func <- function(recon_x, x, mu, logvar) {
  MSE <- nnf_mse_loss(recon_x, x, reduction = "sum")
  KLD <- -0.5 * torch_sum(1 + logvar - torch_pow(mu, 2) - torch_exp(logvar))
  return(MSE + KLD)
}

# 4. Training Loop with Normalized Loss Tracking
num_epochs <- 200
batch_size <- 64
num_train <- nrow(train_tensor_func)

train_norm_loss_hist <- numeric(num_epochs)
test_norm_loss_hist  <- numeric(num_epochs)

print("Training Functional VAE with Normalized Loss...")

for (epoch in 1:num_epochs) {
  
  # Training Phase
  model_func$train()
  indices <- sample(1:num_train)
  train_loss_sum <- 0
  
  for (i in seq(1, num_train, by = batch_size)) {
    end_idx <- min(i + batch_size - 1, num_train)
    batch_idx <- indices[i:end_idx]
    batch_x <- train_tensor_func[batch_idx, ]
    
    optimizer_func$zero_grad()
    
    output <- model_func(batch_x)
    # Calculate raw MSE for tracking
    batch_mse <- nnf_mse_loss(output[[1]], batch_x, reduction = "sum")
    
    # Calculate Backprop Loss (MSE + KLD)
    loss <- vae_loss_func(output[[1]], batch_x, output[[2]], output[[3]])
    loss$backward()
    
    train_loss_sum <- train_loss_sum + batch_mse$item()
    optimizer_func$step()
  }
  
  # Validation Phase
  model_func$eval()
  test_loss_sum <- 0
  
  with_no_grad({
    output_test <- model_func(test_tensor_func)
    batch_test_mse <- nnf_mse_loss(output_test[[1]], test_tensor_func, reduction = "sum")
    test_loss_sum <- batch_test_mse$item()
  })
  
  # Calculate Normalized Loss (MSE / Total Sum of Squares)
  # Unexplained Variance Ratio
  train_tss <- sum((as.matrix(train_tensor_func) - mean(as.matrix(train_tensor_func)))^2)
  test_tss  <- sum((as.matrix(test_tensor_func)  - mean(as.matrix(test_tensor_func)))^2)
  
  train_norm_loss <- train_loss_sum / train_tss
  test_norm_loss  <- test_loss_sum  / test_tss
  
  train_norm_loss_hist[epoch] <- train_norm_loss
  test_norm_loss_hist[epoch]  <- test_norm_loss
  
  if (epoch %% 10 == 0) {
    cat(sprintf("Epoch %d | Norm Train: %.4f | Norm Test: %.4f\n", 
                epoch, train_norm_loss, test_norm_loss))
  }
}

# 5. Plot Learning Curve (Normalized)
history_func_df <- data.frame(
  Epoch = 1:num_epochs,
  Train = train_norm_loss_hist,
  Test  = test_norm_loss_hist
) %>%
  pivot_longer(cols = c("Train", "Test"), names_to = "Type", values_to = "NormalizedLoss")

ggplot(history_func_df, aes(x = Epoch, y = NormalizedLoss, color = Type)) +
  geom_line(size = 1) +
  scale_y_continuous(limits = c(0, 1)) + 
  scale_color_manual(values = c("Train" = "#1f77b4", "Test" = "#d62728")) +
  labs(title = "Functional VAE Learning Curve",
       subtitle = "Proportion of Unexplained Variance (Lower is Better)",
       y = "Normalized Loss (1 - R^2)") +
  theme_minimal()

# 6. Report Card (Original Scale)
model_func$eval()
with_no_grad({
  output <- model_func(test_tensor_func)
  recon_log_scaled <- as.matrix(output[[1]])
  actual_log_scaled <- as.matrix(test_tensor_func)
})

# Inverse Transform Logic
recon_raw  <- expm1(recon_log_scaled * max_val_func)
actual_raw <- expm1(actual_log_scaled * max_val_func)

mse_raw <- mean((recon_raw - actual_raw)^2)
rmse_raw <- sqrt(mse_raw)
total_var_raw <- var(as.vector(actual_raw))
r2_raw <- 1 - (mse_raw / total_var_raw)


print(paste("Average Error (RMSE):", round(rmse_raw, 4), "strength units"))
print(paste("Variance Explained (R2):", round(r2_raw * 100, 2), "%"))

# 7. Extract Features & Merge
model_func$eval()
final_out_func <- model_func(x_tensor_func)
vae_scores_func <- as.matrix(final_out_func[[2]])

vae_func_df <- data.frame(Subject = fc_ids_clean, vae_scores_func)
colnames(vae_func_df)[2:(latent_dim+1)] <- paste0("VAE_Func_LD", 1:latent_dim)
```


To extract non-linear biomarkers, we utilized the encoder network of our trained Variational Autoencoder (VAE) to map each subject's high-dimensional connectome (2,278 features) into a lower-dimensional latent distribution. The encoder defines this distribution using two learned parameters for each dimension: a mean ($\mu$) representing the coordinate center, and a variance ($\sigma$) representing uncertainty. While $\sigma$ is used during training to enforce manifold smoothness via stochastic sampling, it primarily captures noise and reconstruction ambiguity. Therefore, for downstream analysis, we discarded the uncertainty term and extracted the deterministic mean vector ($\mu$) as the final subject representation. These values function analogously to Principal Component scores, providing a stable, denoised summary of the intrinsic topological structure of each brain.

# Merge data and output into csv
```{r}
vae_df <- vae_struct_df %>%
  inner_join(vae_func_df, by = "Subject")
write.csv(vae_df, "Data/vae_df.csv")
```





