---
title: "pca-vae"
author: "Sean Shen"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(R.matlab)
library(tidyverse)
```


```{r torch setup}
if (!requireNamespace("torch", quietly = TRUE)) {
  install.packages("torch")
}

# 2. Load the library
library(torch)

# 3. Check if the C++ backend (Lantern) is installed
# If not, install it automatically.
if (!torch::torch_is_installed()) {
  torch::install_torch()
}

# might need vc_redist.x64.exe on windows
```


# Principal Component Analysis


## Structural Connectome
```{r struct}
# Load Raw Structural Data
# This is the 68x68x1065 Tensor
struct_raw <- readMat("Data/SC/HCP_cortical_DesikanAtlas_SC.mat")
sc_tensor  <- struct_raw$hcp.sc.count
sc_ids     <- as.vector(struct_raw$all.id)

n_subs <- length(sc_ids)
# There are 68x67/2 = 2278 unique connections in a symmetric matrix
n_feats <- 68 * 67 / 2 

# Flatten the 3D Tensor into a 2D Matrix (Subjects x Connections)
# We only take the upper triangle to avoid duplicates
sc_flat <- matrix(NA, nrow = n_subs, ncol = n_feats)

print("Flattening Structural Data...")
for (i in 1:n_subs) {
  # Extract single subject matrix
  mat <- sc_tensor[,,i]
  # Flatten upper triangle
  sc_flat[i, ] <- mat[upper.tri(mat)]
}


# Remove Zero Variance Columns
# We calculate variance for every column
col_vars <- apply(sc_flat, 2, var)

# We identify columns where variance is 0 (Constant columns)
const_cols <- which(col_vars == 0)

print(paste("Removing", length(const_cols), "constant columns..."))

# We subset the matrix to keep only columns with variance > 0
if (length(const_cols) > 0) {
  sc_flat_clean <- sc_flat[, -const_cols]
} else {
  sc_flat_clean <- sc_flat
}

# Run Standard PCA
# rank. = 60 keeps only the top 60 components to match TNPCA
print("Running Structural PCA...")
sc_pca <- prcomp(sc_flat_clean, center = TRUE, scale. = TRUE, rank. = 60)

# Create Dataframe
# We name them "Raw_Struct_PC" to distinguish from the TNPCA "Struct_PC"
my_struct_df <- data.frame(Subject = sc_ids, sc_pca$x)
colnames(my_struct_df)[2:61] <- paste0("Raw_Struct_PC", 1:60)
```

## Functional Connectome

```{r func}
library(R.matlab)
library(dplyr)

# --- Load Raw Functional Data ---
func_raw <- readMat("Data/FC/HCP_cortical_DesikanAtlas_FC.mat")
fc_list  <- func_raw$hcp.cortical.fc
fc_ids   <- as.vector(func_raw$subj.list)

n_subs_fc <- length(fc_ids)
n_feats <- 68 * 67 / 2 

# Initialize with NA (So bad subjects just stay as NA)
fc_flat <- matrix(NA, nrow = n_subs_fc, ncol = n_feats)

print("Flattening Functional Data (with Error Checking)...")
for (i in 1:n_subs_fc) {
  # Extract matrix
  mat <- fc_list[[i]]
  if (is.list(mat)) { mat <- mat[[1]] } 
  
  # If matrix is actually 68x68
  if (all(dim(mat) == c(68, 68))) {
    fc_flat[i, ] <- mat[upper.tri(mat)]
  } else {
    # If bad (0x0), we do nothing. The row stays as NA.
    print(paste("Skipping Subject Index:", i, "- Empty/Bad Matrix"))
  }
}

# --- Drop Bad Subjects ---
# We verify which rows are still NA and remove them
bad_rows <- which(is.na(fc_flat[,1]))
if (length(bad_rows) > 0) {
  print(paste("Removing", length(bad_rows), "failed subjects."))
  fc_flat_clean <- fc_flat[-bad_rows, ]
  fc_ids_clean  <- fc_ids[-bad_rows]
} else {
  fc_flat_clean <- fc_flat
  fc_ids_clean  <- fc_ids
}

# --- Remove Zero Variance Columns ---
col_vars_fc <- apply(fc_flat_clean, 2, var)
const_cols_fc <- which(col_vars_fc == 0)

if (length(const_cols_fc) > 0) {
  fc_flat_final <- fc_flat_clean[, -const_cols_fc]
} else {
  fc_flat_final <- fc_flat_clean
}

# --- Run PCA ---
print("Running Functional PCA...")
fc_pca <- prcomp(fc_flat_final, center = TRUE, scale. = TRUE, rank. = 60)

# --- Create Dataframe ---
my_func_df <- data.frame(Subject = fc_ids_clean, fc_pca$x)
colnames(my_func_df)[2:61] <- paste0("Raw_Func_PC", 1:60)

print("Functional PCA Complete.")
```



```{r}
write.csv(my_struct_df, "Data/raw_struct_pca.csv")
write.csv(my_func_df, "Data/raw_func_pca.csv")
```


# Variational Autoencoder

## Structural Connectome
```{r}
library(torch)
library(dplyr)

# Prepare Data
x_mat <- scale(sc_flat_clean)
x_tensor <- torch_tensor(x_mat, dtype = torch_float())

# Split Data (80% Train, 20% Test)
set.seed(123)
torch_manual_seed(123)
sample_idx <- sample(1:nrow(x_tensor), size = 0.8 * nrow(x_tensor))
train_tensor <- x_tensor[sample_idx, ]
test_tensor  <- x_tensor[-sample_idx, ] # The held-out set for testing

input_dim <- ncol(x_mat)
latent_dim <- 60  
hidden_dim <- 512

# Define VAE Architecture
vae_module <- nn_module(
  "VAE",
  
  initialize = function(input_dim, hidden_dim, latent_dim) {
    self$fc1 <- nn_linear(input_dim, hidden_dim)
    self$fc_mu <- nn_linear(hidden_dim, latent_dim)
    self$fc_logvar <- nn_linear(hidden_dim, latent_dim)
    self$fc_d1 <- nn_linear(latent_dim, hidden_dim)
    self$fc_d2 <- nn_linear(hidden_dim, input_dim)
  },
  
  encode = function(x) {
    h <- torch_relu(self$fc1(x))
    mu <- self$fc_mu(h)
    logvar <- self$fc_logvar(h)
    return(list(mu, logvar))
  },
  
  reparameterize = function(mu, logvar) {
    std <- torch_exp(0.5 * logvar)
    eps <- torch_randn_like(std)
    return(mu + eps * std)
  },
  
  decode = function(z) {
    h <- torch_relu(self$fc_d1(z))
    return(self$fc_d2(h))
  },
  
  forward = function(x) {
    enc_out <- self$encode(x)
    mu <- enc_out[[1]]
    logvar <- enc_out[[2]]
    z <- self$reparameterize(mu, logvar)
    recon_x <- self$decode(z)
    return(list(recon_x, mu, logvar, z))
  }
)

# Initialize Model & Optimizer
model <- vae_module(input_dim, hidden_dim, latent_dim)
optimizer <- optim_adam(model$parameters, lr = 0.001, weight_decay = 0.0005)

# Loss Function
vae_loss <- function(recon_x, x, mu, logvar) {
  MSE <- nnf_mse_loss(recon_x, x, reduction = "sum")
  KLD <- -0.5 * torch_sum(1 + logvar - torch_pow(mu, 2) - torch_exp(logvar))
  return(MSE + KLD)
}

# Training Loop
num_epochs <- 200
batch_size <- 64
num_train <- nrow(train_tensor)

print("Training VAE with Validation...")

for (epoch in 1:num_epochs) {
  
  # --- Training Phase ---
  model$train() # Set to training mode (enables gradients/dropout)
  indices <- sample(1:num_train)
  train_loss_total <- 0
  
  for (i in seq(1, num_train, by = batch_size)) {
    end_idx <- min(i + batch_size - 1, num_train)
    batch_idx <- indices[i:end_idx]
    batch_x <- train_tensor[batch_idx, ]
    
    optimizer$zero_grad()
    
    # Forward & Backward
    output <- model(batch_x)
    loss <- vae_loss(output[[1]], batch_x, output[[2]], output[[3]])
    loss$backward()
    train_loss_total <- train_loss_total + loss$item()
    optimizer$step()
  }
  
  # --- Validation Phase (Test Loss) ---
  # We run this every 10 epochs to save time
  if (epoch %% 10 == 0) {
    model$eval() # Set to evaluation mode (disables gradients)
    test_loss_total <- 0
    
    # We use 'with_no_grad' to speed up calculation and save memory
    with_no_grad({
      output_test <- model(test_tensor)
      loss_test <- vae_loss(output_test[[1]], test_tensor, output_test[[2]], output_test[[3]])
      test_loss_total <- loss_test$item()
    })
    
    # Calculate Averages
    avg_train_loss <- train_loss_total / num_train
    avg_test_loss  <- test_loss_total / nrow(test_tensor)
    
    cat(sprintf("Epoch %d | Train Loss: %.2f | Test Loss: %.2f\n", 
                epoch, avg_train_loss, avg_test_loss))
  }
}

# Extract Latent Variables (Using ALL data for the final features)
# We switch back to eval mode to get deterministic features
model$eval()
final_out <- model(x_tensor)
vae_scores <- as.matrix(final_out[[2]]) # Extract Mu

# Create Dataframe
vae_struct_df <- data.frame(Subject = sc_ids, vae_scores)
colnames(vae_struct_df)[2:(latent_dim+1)] <- paste0("VAE_Struct_LD", 1:latent_dim)
```



## Functional Connectome


```{r}
library(torch)
library(dplyr)

# Prepare Functional Data
# We assume fc_flat_final exists from your previous Functional PCA step
# Z-score normalize the functional connectivity data
x_mat_func <- scale(fc_flat_final)
x_tensor_func <- torch_tensor(x_mat_func, dtype = torch_float())

# Split Data (80% Train, 20% Test)
set.seed(123)
torch_manual_seed(123)
sample_idx <- sample(1:nrow(x_tensor_func), size = 0.8 * nrow(x_tensor_func))
train_tensor_func <- x_tensor_func[sample_idx, ]
test_tensor_func  <- x_tensor_func[-sample_idx, ]

# Dimensions
input_dim <- ncol(x_mat_func)
latent_dim <- 60
hidden_dim <- 512

# Define VAE Architecture
# This is identical to the Structural one to keep the comparison fair
vae_module_func <- nn_module(
  "VAE_Func",
  
  initialize = function(input_dim, hidden_dim, latent_dim) {
    self$fc1 <- nn_linear(input_dim, hidden_dim)
    self$fc_mu <- nn_linear(hidden_dim, latent_dim)
    self$fc_logvar <- nn_linear(hidden_dim, latent_dim)
    self$fc_d1 <- nn_linear(latent_dim, hidden_dim)
    self$fc_d2 <- nn_linear(hidden_dim, input_dim)
  },
  
  encode = function(x) {
    h <- torch_relu(self$fc1(x))
    mu <- self$fc_mu(h)
    logvar <- self$fc_logvar(h)
    return(list(mu, logvar))
  },
  
  reparameterize = function(mu, logvar) {
    std <- torch_exp(0.5 * logvar)
    eps <- torch_randn_like(std)
    return(mu + eps * std)
  },
  
  decode = function(z) {
    h <- torch_relu(self$fc_d1(z))
    return(self$fc_d2(h))
  },
  
  forward = function(x) {
    enc_out <- self$encode(x)
    mu <- enc_out[[1]]
    logvar <- enc_out[[2]]
    z <- self$reparameterize(mu, logvar)
    recon_x <- self$decode(z)
    return(list(recon_x, mu, logvar, z))
  }
)

# Initialize Model and Optimizer
model_func <- vae_module_func(input_dim, hidden_dim, latent_dim)
optimizer_func <- optim_adam(model_func$parameters, lr = 0.001, weight_decay = 0.0005)

# Loss Function
vae_loss_func <- function(recon_x, x, mu, logvar) {
  MSE <- nnf_mse_loss(recon_x, x, reduction = "sum")
  KLD <- -0.5 * torch_sum(1 + logvar - torch_pow(mu, 2) - torch_exp(logvar))
  return(MSE + KLD)
}

# Training Loop
num_epochs <- 200
batch_size <- 64
num_train <- nrow(train_tensor_func)

print("Training Functional VAE...")

for (epoch in 1:num_epochs) {
  
  # Training Phase
  model_func$train()
  indices <- sample(1:num_train)
  train_loss_total <- 0
  
  for (i in seq(1, num_train, by = batch_size)) {
    end_idx <- min(i + batch_size - 1, num_train)
    batch_idx <- indices[i:end_idx]
    batch_x <- train_tensor_func[batch_idx, ]
    
    optimizer_func$zero_grad()
    
    output <- model_func(batch_x)
    loss <- vae_loss_func(output[[1]], batch_x, output[[2]], output[[3]])
    loss$backward()
    train_loss_total <- train_loss_total + loss$item()
    optimizer_func$step()
  }
  
  # Validation Phase
  if (epoch %% 10 == 0) {
    model_func$eval()
    test_loss_total <- 0
    
    with_no_grad({
      output_test <- model_func(test_tensor_func)
      loss_test <- vae_loss_func(output_test[[1]], test_tensor_func, output_test[[2]], output_test[[3]])
      test_loss_total <- loss_test$item()
    })
    
    avg_train_loss <- train_loss_total / num_train
    avg_test_loss  <- test_loss_total / nrow(test_tensor_func)
    
    cat(sprintf("Epoch %d | Train Loss: %.2f | Test Loss: %.2f\n", 
                epoch, avg_train_loss, avg_test_loss))
  }
}

# Extract Latent Variables
model_func$eval()
final_out_func <- model_func(x_tensor_func)
vae_scores_func <- as.matrix(final_out_func[[2]])

# Create Dataframe
# We use Subject IDs from the cleaned functional dataset
vae_func_df <- data.frame(Subject = fc_ids_clean, vae_scores_func)
colnames(vae_func_df)[2:(latent_dim+1)] <- paste0("VAE_Func_LD", 1:latent_dim)
```


To extract non-linear biomarkers, we utilized the encoder network of our trained Variational Autoencoder (VAE) to map each subject's high-dimensional connectome (2,278 features) into a lower-dimensional latent distribution. The encoder defines this distribution using two learned parameters for each dimension: a mean ($\mu$) representing the coordinate center, and a variance ($\sigma$) representing uncertainty. While $\sigma$ is used during training to enforce manifold smoothness via stochastic sampling, it primarily captures noise and reconstruction ambiguity. Therefore, for downstream analysis, we discarded the uncertainty term and extracted the deterministic mean vector ($\mu$) as the final subject representation. These values function analogously to Principal Component scores, providing a stable, denoised summary of the intrinsic topological structure of each brain.

# Merge data and output into csv
```{r}
vae_df <- vae_struct_df %>%
  inner_join(vae_func_df, by = "Subject")
write.csv(vae_df, "Data/vae_df.csv")
```





